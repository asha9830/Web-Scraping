# -*- coding: utf-8 -*-
"""Vertical_Search_Engine_IR.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13AuhHTQAekQ_edtwxA8b9ctefznAJSwj

**Importing the Libraries to Program**
"""

import csv
import pandas as pd
import numpy as np
import os
import requests
from bs4 import BeautifulSoup
from nltk.stem.porter import PorterStemmer
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import snowball
import re
import string
import json

"""**Downloading NLTK Dependencies**"""

nltk.download('punkt')
nltk.download('stopwords')

"""**Extracting the data from Web**"""

def Paper_Search_Coventry(Source_Url_Used, Data):

    Writing_Into_the_File = []
    Counter= 0

    Source_Url_Used = Source_Url_Used + '/publications/'

    Phrase_Used = [Source_Url_Used]

    Source_Url_For_Search = "https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/"

    while(Phrase_Used!=[] and Counter < Data):
        Coven_Uni_Phrase = Phrase_Used[0]
        print(Coven_Uni_Phrase)
        Phrase_Used= Phrase_Used[1:]
        Counter_New=0

        #Reading HTML text and parsing it
        Reading_Text_String = requests.get(Coven_Uni_Phrase)
        Soup_Data = BeautifulSoup(Reading_Text_String.content, "html.parser")
        print("Soup Object")

        Final_Data=Soup_Data.find_all("div",{'class':"result-container"})

        #reading information about journals such as title of journal,
        for info in Final_Data:
            #checking author is from CU or not
            Author_Name=[]
            Author_Link=[]

            Seaparate_User={}

            Author_Details= info.find_all("a", class_="link person")

            for a in Author_Details:
                if(a.get('href')!=[]):
                    Paper_Link = a.get('href')
                    Author=a.string
                    print(Author)

                    Author_Name.append(Author)

                    Author_Link.append(Paper_Link)
                    Seaparate_User['Authors']=Author_Name
                    Seaparate_User['Profile']=Author_Link

            if(Author_Name!=[]):
                Paper_Name= info.find("h3", class_="title").text
                #print(Paper_Name)
                Seaparate_User['Journal name']=Paper_Name
                Counter_New = Counter_New + 1
                print(Counter_New)

                Research_Paper_Link = info.find("a", class_="link").get('href')
                #print(Research_Paper_Link)
                Seaparate_User['Journal URL']= Research_Paper_Link

                Paper_Published_Date=info.find("span", class_="date").text
                #print(Paper_Published_Date)
                Seaparate_User['Publishing Date']= Paper_Published_Date

                Writing_Into_the_File.append(Seaparate_User)

        #Next page link
        Next_Page=Soup_Data.find("a", class_="nextLink")

        if(Next_Page!= None):

            Next_Page_Link = Next_Page.get('href')

            Next_Page_Link = Source_Url_For_Search + '/' + Next_Page_Link.split('/')[4] + '/' + Next_Page_Link.split('/')[5]

            print("Link added to queue for next page is: ", Next_Page_Link)

            Phrase_Used.append(Next_Page_Link)

        #making outputs csv file
        Output_Data = open('Research_Paper.csv', 'w', encoding="utf-8")
        labels=['Authors','Profile','Journal name','Journal URL','Publishing Date']

        Writering_To_Excel = csv.DictWriter(Output_Data, fieldnames=labels)
        Writering_To_Excel.writeheader()

        for r in Writing_Into_the_File:
            Writering_To_Excel.writerow(r)
        Output_Data.close()

        Counter += 1

"""**Web Crawling**"""

acc_inpt=int(input("Press 1 to run crawler and 2 to continue from saved output file"))
if (acc_inpt==1):
    Paper_Search_Coventry('https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell',60)
elif (acc_inpt==2):
    if(os.path.exists("Research_Paper.csv")):
        print("Search started from initially saved Crawler optput")
    else:
        print("You need to crawler first.")
else:
       print("invalid response.")

"""**Reading the Saved into DataFrame**"""

Scraped_Data_Of_Paper = pd.read_csv("Research_Paper.csv")
Scraped_Data_Of_Paper.info()
Scraped_Data_Of_Paper

"""**Data Preprocessing**"""

#Pre-processing
import re
import string
Searching_Paper = []
for Paper_Search in Scraped_Data_Of_Paper['Journal name']:
    # Removing Unicode
    Paper_Title = re.sub(r'[^\x00-\x7F]+', '', Paper_Search)
    # Removing Mentions
    Paper_Title= re.sub(r'@\w+', '', Paper_Title)
    # Converting into lowercase
    Paper_Title = Paper_Title.lower()
    # Removing punctuations
    Paper_Title = re.sub(r'[%s]' % re.escape(string.punctuation), '', Paper_Title)
    #removing stop words
    stp = re.compile(r'\b(' + r'|'.join(stopwords.words('english')) + r')\b\s*')
    text = stp.sub('',Paper_Title)
    Searching_Paper.append(text)

print(Searching_Paper)

"""**Data Tokenisation and Stemming (Removing Stop Words)**"""

#Stemming and removing stop words
Remove_Stop_Words=[]
for i in Searching_Paper:
    Token_Words=word_tokenize(i)
    Stem_Words=[]
    for word in Token_Words:
        if word not in stopwords.words("english"):
            if(word.isalpha()):
                stemmer = PorterStemmer()
                Stem_Words.append(stemmer.stem(word))
                doc=' '.join(Stem_Words)
    Remove_Stop_Words.append(doc)
print(Remove_Stop_Words)

"""**Indexing the Data**"""

# Indexer part
def build_indexer(data):
    word_to_docs = {}
    for i, doc_val in enumerate(data['Journal name'].values):
        doc_val = Searching_Paper
        tokens = Token_Words
        stemmed_tokens = Remove_Stop_Words
        for token in stemmed_tokens:
            word_to_docs[token].append(i+1)
    return word_to_docs
word_to_docs = build_indexer(Scraped_Data_Of_Paper)

"""**Transforming into Vextorization**"""

from sklearn.feature_extraction.text import TfidfVectorizer
Transform_Pro = TfidfVectorizer()
Pro_Vector = Transform_Pro .fit_transform(Remove_Stop_Words)
Feature_Names = Transform_Pro.get_feature_names_out()
Pr_Dense = Pro_Vector.todense()
Pr_Dense_List =Pr_Dense.tolist()
Pr_Tranform_Fit = pd.DataFrame(Pr_Dense_List, columns=Feature_Names)

Pr_Tranform_Fit

"""**Querring and Searching the Document**"""

Search_Word = True
while Search_Word:
    Result = {}
    Input_Data = input('Enter a string: ')
    if Input_Data == 'NULL':
        break
    print('\n')
    Tokenised_Words = word_tokenize(Input_Data)
    Stem_Sentence = []
    for word in Tokenised_Words:
        Stem_Sentence.append(stemmer.stem(word))
    Search_Word = ' '.join(Stem_Sentence)
    Input_Data = Search_Word.split()
    print(Input_Data)
    print('\n')
    Sum = list()

    for i in range(len(Remove_Stop_Words)):
        Total = 0
        for j in range(len(Input_Data)):
            try:
                t = Input_Data[j]
                Total += Pr_Tranform_Fit.iloc[i][t]
            except KeyError:
                Total += 0
        Sum.insert(i, Total)

    Max_value = max(Sum)
    a = sorted([(x, i) for (i, x) in enumerate(Sum)], reverse=True)[:3]
    pd.set_option('display.max_colwidth', None)
    Results_Found = False
    for i in range(len(a)):
        Max_Index = a[i][1]
        if Sum[Max_Index] != 0:
            Results_Found = True
            print(Scraped_Data_Of_Paper.iloc[Max_Index])
            print('\n')
    if not Results_Found:
        print("No results found for the given search term.")

